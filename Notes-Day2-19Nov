Start the Lab

# sudo su -

# kubectl get nodes

demo1: recreate Deployment Strategy:

# vim deployment-recreate.yml

apiVersion: apps/v1
kind: Deployment
metadata:
 name: mydeploy
spec:
 replicas: 3
 strategy:
  type: Recreate
 selector:
  matchLabels:
   app: kubeserve
 template:
  metadata:
   labels:
    app: kubeserve
  spec:
   containers:
    - name: app
      image: leaddevops/kubeserve:v1



29  kubectl create -f recreate-deployment.yml
   33  kubectl get all
   35  kubectl get deployment -o wide
   36  kubectl set image deployment mydeploy app=leaddevops/kubeserve:v2

  37  kubectl get all
   38  kubectl get deployment -o wide


Demo2: Rolling update Deployment

===============================

# kubectl delete all --all

Create the deployment and service object

...

kubectl create -f https://raw.githubusercontent.com/Sonal0409/Vodafone-Kubernetes-Notes/main/Day2-Notes/Deployment/deployment.yml
...

kubectl get deployment
kubectl get service
kubectl get pods
kubectl scale deployment kubeserve --replicas=5
kubectl get pods
kubectl scale deployment kubeserve --replicas=2
Change the Image

kubectl set image deployment kubeserve app=leaddevops/kubeserve:v2
kubectl rollout status
kubectl set image deployment kubeserve app=leaddevops/kubeserve:v3
kubectl rollout status deployment kubeserve
V3 is faulty.. so rollback to previous version

kubectl rollout undo deployment kubeserve
kubectl rollout history deployment kubeserve
kubectl rollout history deployment kubeserve --revision=3
Note:
Run the loop to send continous request to service object

Go to worker node and execute below command:

while true;do curl serviceClusterIP:80;sleep 1;echo"";done

example: while true;do curl 10.102.253.113:80;sleep 1;echo"";done

======================================================================
Blue and Green Deployment:
========================================

# kubectl delete all --all

Demo1:
=====================

vim deployment-blue.yml

apiVersion: apps/v1
kind: Deployment
metadata:
 name: blue
spec:
 replicas: 3
 selector:
  matchLabels:
   app: kubeserve
   env: blue
 template:
  metadata:
   labels:
    app: kubeserve
    env: blue
  spec:
   containers:
    - name: c1
      image: leaddevops/kubeserve:v1



# kubectl delete deployment mydeploy
# kubectl create -f deployment-blue.yml
# kubectl get all

Create a Service for Blue environment
========================


# vim service-blue.yml



apiVersion: v1
kind: Service
metadata:
 name: blue
spec:
 type: ClusterIP
 ports:
  - targetPort: 80
    port: 80
 selector:
  app: kubeserve
  env: blue


# kubectl create -f service-blue.yml
# kubectl get endpoints

Observe: Traffic being redirected to blue environment pods

Set up green Deployment
======================================

vim deployment-green.yml

apiVersion: apps/v1
kind: Deployment
metadata:
 name: green
spec:
 replicas: 0
 selector:
  matchLabels:
   app: kubeserve
   env: green
 template:
  metadata:
   labels:
    app: kubeserve
    env: green
  spec:
   containers:
    - name: c1
      image: leaddevops/kubeserve:v2


# kubectl create -f deployment-green.yml

# kubectl get deployments

You will see zero pods for green deployment right now


Write the service yAMl

# vim service-green.yml

apiVersion: v1
kind: Service
metadata:
 name: green
spec:
 type: ClusterIP
 ports:
  - targetPort: 80
    port: 80
 selector:
  app: kubeserve
  env: green


We are not creating the service right now.


We will first scale up the green deployment, observe the pods are running


# kubectl scale deployment green  --replicas=3

# kubectl create -f service-green.yml


Scale down the blue deployment

# kubectl scale deployment blue --replicas=0

# kubectl get endpoints


=======================================================================

Canary Deployment
================================

Create a Base deployment fo v1 of the application

# vim deployment-base.yml

apiVersion: apps/v1
kind: Deployment
metadata:
 name: base-app
spec:
 replicas: 3
 selector:
  matchLabels:
   app: kubeserve
 template:
  metadata:
   labels:
    app: kubeserve
  spec:
   containers:
    - name: c1
      image: leaddevops/kubeserve:v1



# kubectl create -f deployment-base.yml

# kubectl get deployments

Create service for base deployment

# vim service-base.yml

apiVersion: v1
kind: Service
metadata:
 name: canary-service
spec:
 type: ClusterIP
 ports:
  - targetPort: 80
    port: 80
 selector:
  app: kubeserve


Create the new deployment with version 2

# vim deployment-new.yml

apiVersion: apps/v1
kind: Deployment
metadata:
 name: new-app
spec:
 replicas: 1
 selector:
  matchLabels:
   app: kubeserve
 template:
  metadata:
   labels:
    app: kubeserve
  spec:
   containers:
    - name: c1
      image: leaddevops/kubeserve:v2


  100  kubectl create -f deployment-new.yml
  101  kubectl get endpoints
  102  clear
  103  kubectl scale deployment new-app  --replicas=3
  104  kubectl scale deployment base-app  --replicas=0
  105  kubectl get deployment

Horizontal Pod Autoscaler
========================================================

Instal Metric Server:

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

wget -c  https://gist.githubusercontent.com/initcron/1a2bd25353e1faa22a0ad41ad1c01b62/raw/008e23f9fbf4d7e2cf79df1dd008de2f1db62a10/k8s-metrics-server.patch.yaml

kubectl patch deploy metrics-server -p "$(cat k8s-metrics-server.patch.yaml)" -n kube-system



# kubectl get pods -n kube-system

# cd

# cd mykubefiles

# vim hpa.yml

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginxpod
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          resources:
            limits:
              cpu: 10m

---

apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
spec:
  type: ClusterIP  ## this is default if we do not type in service definition
  selector:
    app: nginx
  ports:
   - protocol: TCP
     port: 80
     targetPort: 80

nginx-svc        ClusterIP   10.103.99.235   <none>        80/TCP    2s
---

apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 5



Save the file

# kubectl delete all --all

# kubectl create -f hpa.yml

# kubectl get all

# Now launch the load generator pod -> open master terminal again

# kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://Service-Internal-IP:80; done"

Check the pods:

kubectl get pods
kubectl top pods

===============================================================
VPA:
=====================================

# git clone https://github.com/kubernetes/autoscaler.git

# cd autoscaler

# cd vertical-pod-autoscaler

# ./hack/vpa-up.sh
# ./pkg/admission-controller/gencerts.sh

# kubectl get pods -n kube-system | grep vpa

# cd

# vim deployment-vpa.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: high-cpu-utilization-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cpu-utilization-app
  template:
    metadata:
      labels:
        app: cpu-utilization-app
    spec:
      containers:
      - name: cpu-utilization-container
        image: ubuntu
        command: ["/bin/sh", "-c", "apt-get update && apt-get install -y stress-ng && while true; do stress-ng --cpu 1; done"]
        resources:
          limits:
            cpu: "0.05"
          requests:
            cpu: "0.05"

save the file

# kubectl create -f deployment-vpa.yml


# vim vpa.yml

apiVersion: "autoscaling.k8s.io/v1"
kind: VerticalPodAutoscaler
metadata:
  name: stress-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: high-cpu-utilization-deployment
  updatePolicy:
    updateMode: Auto
  resourcePolicy:
    containerPolicies:
      - containerName: '*'
        minAllowed:
          cpu: 100m
          memory: 50Mi
        maxAllowed:
          cpu: 200m  #maximum vpa will be allocating this many cpus even if demand is higher.
          memory: 500Mi
        controlledResources: ["cpu", "memory"]


save the file

# kubectl create -f vpa.yml

# kubectl top pods

# kubectl get pods














