Create Daemon Set in the cluster

kubectl create -f https://raw.githubusercontent.com/Sonal0409/Vodafone-Kubernetes-Notes/main/Day2-Notes/DaemonSetDemo/deamonset.yml

Validate if single pod is created on every worker node
kubectl get pods -o wide

You will see 2 pods have been created on each Node

kubectl get ds

A daemonset is created with desired pods as 2
Note:
If a new node is added to the cluster,a new pod will be created on the new node also. Desired count would have increased to 3.

======================================


Jobs in kubernetes:
===================================================
A job is a controller in kubernetes which will create a pod.
In the pod a job/task will be executed. As the task is completed, the pod status will be completed and the job will stop.
As part of the Job, the pods are alive only until the task is completed.

As part of job only 1 pod is created by default.
If the pod fails to complete the task,Job will never restart the pod
Job will create a new pod to execute the task.
On the job we can set up number of retries to be done in case the pods are failing using the parameter:
 backoffLimit: 4

As part of the job we can also create multiple pods 
Using the parameter: completions = 4
I.e. 4 pods have to be created and tasks has to be complete din them
These pods will be created by default one after the other.

However we can setup parameter -> parallelism 
Ex: parallelism=2 
So 2 pods will be created parallely.


vim job.yml

apiVersion: batch/v1
kind: Job
metadata:
  name: pi1
spec:
  backoffLimit: 4
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]

kubectl apply -f job.yaml
kubectl get jobs -o wide
kubectl get pods -o wide
kubectl describe pod pi-wf7bm

kubectl logs pi-27zzb


Demo 2: Make an error on the command in job YAMl and create a new job

Observe: due to backofflimit = 4 , it will try to create the pod again 4 more times.

Total 5 pods. 1 initial pod and 4 retry pods

There will not be any restart of the pod in concept of job


# vim job2Err.yml

apiVersion: batch/v1
kind: Job
metadata:
  name: pi1
spec:
  backoffLimit: 4
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["pel",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]


================================

Cron Job:
===========================================

# vim cronjob.yml

apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      backoffLimit: 4
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster

# kubectl create -f cronjob.yml
# kubectl get cronjob
# kubectl get cronjob hello
# kubectl get jobs --watch
# kubectl get cronjob hello

================================

Static Pods
****************

Which process in kubernetes is responsible to create PODs

Kubelet is the process which is used to create and run PODS

By default we send request to apiserver to create pods, but it is kubelet that created pods on the 
scheduled node

So can we directly instruct kubelet to create a pod on the node of a cluster 
Yes that is possible: using the concept of Static pods

- Static pods are created by Kubelet directly
- We should give Pod yaml to kubelet so that kubelet can create pod
- if you have to give YAML to kubelet Then we have to logon to the node where the pods has to be created
- kubelet process running on the node reads the YAMl file from a location/path on that node or VM
where we can place the yaml
- By default kubelet has a configuration path called StaticPODpath
- When Pod created by the kubelet it appends the node name to the podname in the YAML

Example:
*****************

Go to a worker node:

execute following command:

# service kubelet status

Now go to this location

# cd /etc/kubernetes/manifests

Create  a file with name as 

vim static-pod.yml

Copy the text from master machine(static-pod.yml file) into this file

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: static-pod
  name: static-pod
spec:
  containers:
  - image: nginx
    name: static-pod



Save the file

*******************

Go to Master node
*****************************

kubectl get pods --> static pod wil be there , here node name will be appeneded to pod name

kubectl get pods -o wide --> pod will be scenduled on the same node by kubelet

Now delete the pods and observe the behaviour

$ kubectl delete pod <podname>

You will observe kubelet will recreaate the pod on itself

The only way to delete static pod is to delete the yaml file on slave node in the directory cd /etc/kubernetes/manifests

go to slave 
************

cd /etc/kubernetes/manifests

rm static-pod.yml

go to master and check the if pod is available or not


same can be done on the master node aslo if you copy the yaml file in the /ect/manifests  path. In this case pod will be create don the master node


If in case the pod is failing and you want to check the kubelet logs on the SLAVE:

# journalctl -u kubelet.service | less

**********************************

What is the use of static pods?

In real time for deployment we don't need static pods, but then why kuberentes team has come up with this Concept.

Lets understand that:

# kubectl get pods -n kube-system -o wide

here these are also all pods of kubernetes system
kubernetes deploys its components as pods, but how did they get created even when we didnot have api server
How did the etcd, apiserver pod get created?
because kube-system pods are all static pods
All the management components are all static pods

only for cluster creation purpose kubvernetes brought in the process of static pods.

On the master machine, if you check the location for manifest path

# cd /etc/kubernetes/manifests/
# ls -l
# you will see all the yaml files here

when we do a kubeadm init command on manual set up

kubernetes will download the yaml files from internet and place them in masfests path and set the kube cluster
