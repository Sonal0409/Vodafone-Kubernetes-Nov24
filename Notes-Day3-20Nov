Create Daemon Set in the cluster

kubectl create -f https://raw.githubusercontent.com/Sonal0409/Vodafone-Kubernetes-Notes/main/Day2-Notes/DaemonSetDemo/deamonset.yml

Validate if single pod is created on every worker node
kubectl get pods -o wide

You will see 2 pods have been created on each Node

kubectl get ds

A daemonset is created with desired pods as 2
Note:
If a new node is added to the cluster,a new pod will be created on the new node also. Desired count would have increased to 3.

======================================


Jobs in kubernetes:
===================================================
A job is a controller in kubernetes which will create a pod.
In the pod a job/task will be executed. As the task is completed, the pod status will be completed and the job will stop.
As part of the Job, the pods are alive only until the task is completed.

As part of job only 1 pod is created by default.
If the pod fails to complete the task,Job will never restart the pod
Job will create a new pod to execute the task.
On the job we can set up number of retries to be done in case the pods are failing using the parameter:
 backoffLimit: 4

As part of the job we can also create multiple pods 
Using the parameter: completions = 4
I.e. 4 pods have to be created and tasks has to be complete din them
These pods will be created by default one after the other.

However we can setup parameter -> parallelism 
Ex: parallelism=2 
So 2 pods will be created parallely.


vim job.yml

apiVersion: batch/v1
kind: Job
metadata:
  name: pi1
spec:
  backoffLimit: 4
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]

kubectl apply -f job.yaml
kubectl get jobs -o wide
kubectl get pods -o wide
kubectl describe pod pi-wf7bm

kubectl logs pi-27zzb


Demo 2: Make an error on the command in job YAMl and create a new job

Observe: due to backofflimit = 4 , it will try to create the pod again 4 more times.

Total 5 pods. 1 initial pod and 4 retry pods

There will not be any restart of the pod in concept of job


# vim job2Err.yml

apiVersion: batch/v1
kind: Job
metadata:
  name: pi1
spec:
  backoffLimit: 4
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["pel",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]


================================

Cron Job:
===========================================

# vim cronjob.yml

apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      backoffLimit: 4
      template:
        spec:
          restartPolicy: Never
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster

# kubectl create -f cronjob.yml
# kubectl get cronjob
# kubectl get cronjob hello
# kubectl get jobs --watch
# kubectl get cronjob hello

================================

Static Pods
****************

Which process in kubernetes is responsible to create PODs

Kubelet is the process which is used to create and run PODS

By default we send request to apiserver to create pods, but it is kubelet that created pods on the 
scheduled node

So can we directly instruct kubelet to create a pod on the node of a cluster 
Yes that is possible: using the concept of Static pods

- Static pods are created by Kubelet directly
- We should give Pod yaml to kubelet so that kubelet can create pod
- if you have to give YAML to kubelet Then we have to logon to the node where the pods has to be created
- kubelet process running on the node reads the YAMl file from a location/path on that node or VM
where we can place the yaml
- By default kubelet has a configuration path called StaticPODpath
- When Pod created by the kubelet it appends the node name to the podname in the YAML

Example:
*****************

Go to a worker node:

execute following command:

# service kubelet status

Now go to this location

# cd /etc/kubernetes

# mkdir manifests

# cd manifests

Create  a file with name as 

vim static-pod.yml

Copy the text from master machine(static-pod.yml file) into this file

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: static-pod
  name: static-pod
spec:
  containers:
  - image: nginx
    name: static-pod



Save the file

*******************

Go to Master node
*****************************

kubectl get pods --> static pod wil be there , here node name will be appeneded to pod name

kubectl get pods -o wide --> pod will be scenduled on the same node by kubelet

Now delete the pods and observe the behaviour

$ kubectl delete pod <podname>

You will observe kubelet will recreaate the pod on itself

The only way to delete static pod is to delete the yaml file on slave node in the directory cd /etc/kubernetes/manifests

go to Worker node 
************

cd /etc/kubernetes/manifests

rm static-pod.yml

go to master and check the if pod is available or not


same can be done on the master node aslo if you copy the yaml file in the /ect/manifests  path. In this case pod will be create don the master node


If in case the pod is failing and you want to check the kubelet logs on the SLAVE:

# journalctl -u kubelet.service | less

**********************************

What is the use of static pods?

In real time for deployment we don't need static pods, but then why kuberentes team has come up with this Concept.

Lets understand that:

# kubectl get pods -n kube-system -o wide

here these are also all pods of kubernetes system
kubernetes deploys its components as pods, but how did they get created even when we didnot have api server
How did the etcd, apiserver pod get created?
because kube-system pods are all static pods
All the management components are all static pods

only for cluster creation purpose kubvernetes brought in the process of static pods.

On the master machine, if you check the location for manifest path

# cd /etc/kubernetes/manifests/
# ls -l
# you will see all the yaml files here

when we do a kubeadm init command on manual set up

kubernetes will download the yaml files from internet and place them in masfests path and set the kube cluster.

======================================
INIT container:

1. A Pod can have multiple containers running apps within it, but it can also have one or more init containers, 
which are run before the app containers are started.

2. Init containers are exactly like regular containers, except:
    > Init containers always run to completion.
    > Each init container must complete successfully before the next one starts.
If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds. 
However, if the Pod has a restartPolicy of Never, 
and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.

3. To specify an init container for a Pod, add the initContainers field into the Pod specification, 
as an array of container items (similar to the app containers field and its contents).

Differences from regular containers:
********************

Init containers support all the fields and features of app containers, 
including resource limits, volumes, and security settings. 
However, the resource requests and limits for an init container are handled differently.

Also, init containers do not support lifecycle, livenessProbe, readinessProbe, or startupProbe 
because they must run to completion before the Pod can be ready.

If you specify multiple init containers for a Pod, kubelet runs each init container sequentially. 
Each init container must succeed before the next can run. 
When all of the init containers have run to completion, kubelet initializes the application containers for the Pod and runs them as usual.


Because init containers have separate images from app containers, they have some advantages for start-up related code:

> Init containers can contain utilities or custom code for setup that are not present in an app image. 
   For example, there is no need to make an image FROM another image just to use a tool like sed, awk, python, or dig during setup.

> Because init containers run to completion before any app containers start, 
init containers offer a mechanism to block or delay app container startup until a set of preconditions are met. 
Once preconditions are met, all of the app containers in a Pod can start in parallel.

> Init containers can securely run utilities or custom code that would otherwise make an app container image less secure. 
By keeping unnecessary tools separate you can limit the attack surface of your app container image.



# vim pod-init.yml


kind: Pod  # kubectl api-resources
apiVersion: v1
metadata:
  name: init-cont-pod1
  namespace: default
  labels:  # are mandatory # helps to indentify a group of pods in cluster # in kube if you do not provide a label kube will assing one automatically
    role: dev  # here both key & value are your choice
spec:
  restartPolicy: Always # Never
  initContainers:
    - name: initcont2
      image: ubuntu
      command: ["bash", "-c", "sleep 5"]
  containers:  # app containers
    - name: tcont
      image: httpd


# kubectl create -f pod-init.yml

# kubectl get pods

================================

Kubernetes Persistent storage:
====================================================

Kubernetes Persistent Volume:
========================

It is nothing but a unit of storage provided by the kubernetes administrator in the cluster

A Persistent volume stores information like:

>  type of volume that kubernetes has access to and you can claim for it
> capacity of volume is available to be claimed
>Status is the volume available or is it blocked, 
> storageClass( Is the PV manually created by running the YAML file or created automatically by kubernetes)


Persistent Volume Claim:
—-------------------------------------

It is a request raised by the admin

A request to block desired volume with desired capacity

In the claim we specify which 
   > volume we want
   > capacity
   > storageClass



Demo 1: Creation of HostPath Volume
=================================

The volume will be created on the VM where the POD scheduled
This is a usecase:
> Where volume is internal to the cluster
> We will manually create the Persistent volume object
> we will raise a claim
> Specify the claim on the POD


# vim pv.yml

apiVersion: v1
kind: PersistentVolume
metadata:
 name: block-pv
spec:
 storageClassName: manual
 capacity:
  storage: 1Gi
 accessModes:
  - ReadWriteOnce
 hostPath:
  path: /tmp/data


# kubectl create -f pv.yml

# vim pvc.yml



apiVersion: v1
kind: PersistentVolumeClaim
metadata:
 name: pvc
spec:
 resources:
  requests:
   storage: 1Gi
 storageClassName: manual
 accessModes:
  - ReadWriteOnce


# kubectl create -f pvc.yml

# kubectl get pv

# kubectl get pvc


# vim pod-pvc.yml

kind: Pod
apiVersion: v1
metadata:
 name: pod-pvc
spec:
 containers:
  - image: nginx
    name: c1
    volumeMounts:  # directory on the container that should preserve daat on the volume
     - mountPath: "/data"
       name: my-volume
 volumes:
  - name: my-volume
    persistentVolumeClaim:
     claimName: pvc



# kubectl create -f pod-pvc.yml
# kubectl get pods -o wide

Go to the worker node where the pods is scenduled and check the volume

# /tmp/data  -> this directory will be there

================================
ConfigMap:

=================================================

ConfigMaps
A ConfigMap is an API object used to store non-confidential data in key-value pairs. Pods can consume ConfigMaps as environment variables, command-line arguments, or as configuration files in a volume.
A ConfigMap allows you to decouple environment-specific configuration from your container images, so that your applications are easily portable.
A ConfigMap is not designed to hold large chunks of data. The data stored in a ConfigMap cannot exceed 1 MiB. If you need to store settings that are larger than this limit, you may want to consider mounting a volume or use a separate database or file service.

ConfigMap does not provide secrecy or encryption. If the data you want to store are confidential, use a Secret rather than a ConfigMap, or use additional (third party) tools to keep your data private.


Demo:
==============

Create 2 configuration file which are storing key and value


# vim dev.properties

app.env: dev
app.mem: 2048
app.url: dev.com

Save the file


# vim prod.properties

app.env: prod
app.mem: 4048
app.url: prod.com

Save the file


Now create 2 different configmaps that store the desired configuration- data

# kubectl create configmap dev-config --from-file=dev.properties

# kubectl describe configmap dev-config

# kubectl create configmap prod-config --from-file=prod.properties

# kubectl describe configmap prod-config

# kubectl get configmap


Now we will create 2 different pods from the same Image that will consume the created configmap.






# vim config-Devpod.yml

apiVersion: v1
kind: Pod
metadata:
 name: dev-pod
spec:
 containers:
  - name: c1
    image: nginx
    volumeMounts:
     - name: config-volume
       mountPath: /etc/config
 volumes:
  - name: config-volume
    configMap:
     name: dev-config



# kubectl create -f config-Devpod.yml

# kubectl exec -it dev-pod -- bash

# cd /etc/config

# ls  ⇒ you will see the dev properties



# vim conf-prodpod.yml

apiVersion: v1
kind: Pod
metadata:
 name: prod-pod
spec:
 containers:
  - name: c1
    image: nginx
    volumeMounts:
     - name: config-volume
       mountPath: /etc/config
 volumes:
  - name: config-volume
    configMap:
     name: prod-config



# kubectl create -f config-prodpod.yml

# kubectl exec -it prod-pod -- bash

# cd /etc/config

# ls  ⇒ you will see the dev properties



Now edit the existing config map for the changes to reflect in container:

#  kubectl edit configmap dev-config -o yaml

It will open a YAMl file.

Press i and make changes to the data

Save the file
Wait for few seconds and then exec into the container

# kubectl exec -it dev-pod -- bash

# cd /etc/config

# cat dev.properties

# exit


