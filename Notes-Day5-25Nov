execute command to get worker node name

# kubectl get nodes
Copy any one worker node name

Scheduling
=====================================================

# vim nodenamedemo.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve  
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubeserve      
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      nodeName: <give worker node name>
      containers:
      - image: leaddevops/kubeserve:v1
        name: app


Save the file

# kubectl create -f nodenamedemo.yml

# kubectl get pods -o wide

Node Selector:
==================================

kubectl label node <nodename1> disk=hdd

kubectl label node <nodename2> disk=ssd

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      nodeSelector:
       disk: ssd
      containers:
      - image: leaddevops/kubeserve:v1
        name: app

======================================================
# kubectl label node <workernode Name> author=sonal

apiVersion: v1
kind: Pod
metadata:
 name: with-node-affinity-1
spec:
 affinity:
  nodeAffinity:
   preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 1
      preference:
       matchExpressions:
        - key: author
          operator: In
          values:
           - sonal
    - weight: 50
      preference:
       matchExpressions:
        - key: disk
          operator: In
          values:
           - hdd
 containers:
  - name: c1
    image: nginx


=======================================
Required during scheduling
==========================================
apiVersion: v1
kind: Pod
metadata:
 name: with-node-affinity
spec:
 affinity:
  nodeAffinity:
   requiredDuringSchedulingIgnoredDuringExecution:
    nodeSelectorTerms:
    - matchExpressions:
       - key: disk
         operator: In
         values:
          - ssd
  containers:
  - name: c1
    image: nginx

=========================================================
TAINTS and TOLERATIONS:
==========================================================

On the master node taint one of the node

kubectl taint node <nodeNmae> color=red:NoSchedule

This node will be tainted

Now try to create any replicas of deployment, no pod will be scehduled on the tainted node.

vim deployment.yml

apiVersion: apps/v1
kind: Deployment
metadata:
 name: blue
spec:
 replicas: 5
 selector:
  matchLabels:
   app: kubeserve
   env: blue
 template:
  metadata:
   labels:
    app: kubeserve
    env: blue
  spec:
   containers:
    - name: c1
      image: leaddevops/kubeserve:v1



This is tainting. 

# kubectl get pods -o wide

Now remove the taint:

kubectl taint node <nodeName> color=red:NoSchedule-


We have 2 types of effect in taint

NoSchedule
NoExecute

Example of NoExecute : in this case the running pods on the node that is tainted will leave the node and get created somewhere else
No new pod will be scheduled on the taineted node as No execute.

example:

Create some replicas and deployemnts, so that pods are there on all nodes

# kubectl delete deployment --all

# kubectl create -f deployment.yml

Now taint one of the node as :

kubectl taint node gke-cluster-1-default-pool-7fe774cd-0m0q color=red:NoExecute

You will observe that pods form tainted node are terminated and recreated.


Tolerations:
*********************


apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubeserve123
spec:
  replicas: 5
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      tolerations:
        - key: color
          operator: "Equal"
          value: "red"
          effect: "NoExecute"
      containers:
      - image: leaddevops/kubeserve:v1
        name: app



Even if the node is tainted with effect as NoExecute, still pod have tolerations to be create don both worker nodes.












